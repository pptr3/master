{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "An **optimization problem** consists of minimizing (maximizing) a real function by choosing finding *best available* values from within an allowed set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum and minimum\n",
    "\n",
    "\n",
    "The value of x for which the first derivative f ’(x) is 0 corresponds to a maximum or a minimum of f(x).\n",
    "\n",
    "• For a **maximum** the second derivative f’’(x) is **negative**.\n",
    "\n",
    "• For a **minimum** the second derivative f’’(x) is **positive**.\n",
    "\n",
    "• The second derivative is 0 for an **inflexion point**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method\n",
    "\n",
    "**Basic formulation**\n",
    "\n",
    "Newton's method finds the roots (zeros) of a function using **linear approximations** of the function.\n",
    "\n",
    "It tries to guess a solution $x_0$ of the equation f(x) = 0.\n",
    "Then, computes the linear approximation (tangent line) of f(x) at $x_0$ and then finds the x-intercept (y=0) of the linear approximation.\n",
    "\n",
    "<img src=\"img/newton.png\">\n",
    "\n",
    "\n",
    "**Newton's method for Optmization**\n",
    "\n",
    "In **optimization**, our goal is to find optima points (minima, maxima). These points are points where the derivative is null. <br>\n",
    "So we look for the **roots of the derivative** (where the derivative is zero, so f'(x) = 0).\n",
    "\n",
    "The roots are known as stationary points of f. <br>\n",
    "These solutions may be minima, maxima, or saddle (inflection) points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "GD is a first-order iterative algorithm for finding the local minimum of a differentiable function f(x).\n",
    "\n",
    "To find a local minimum, it starts from a point x0 and takes steps proportional to the negative of the gradient of the function at the current point.\n",
    "\n",
    "\n",
    "The length of the steps is dictated by a parameter, the **learning rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "In Stochastic Gradient Descent (SGD), only a few samples (a “batch”) are randomly selected at each iteration since GD could be computationally very expensive.\n",
    "\n",
    "\n",
    "Pure Stochastic Gradient Descent uses only a single sample, i.e., a batch size of one, to perform each iteration: many iterations very cheap.\n",
    "\n",
    "\n",
    "**Here is why we are saving time**\n",
    "\n",
    "Suppose we have 1 billion data points.\n",
    "\n",
    "In GD, in order to update the parameters once, we need to have the (exact) gradient. This requires to sum up these 1 billion data points to perform 1 update.\n",
    "\n",
    "In SGD, we can think of it as **trying to get an approximated gradient** instead of exact gradient. The approximation is coming from one data point (or several data points called mini batch). Therefore, in SGD, we can update the parameters very quickly. In addition, if we \"loop\" over all data (called one epoch), we actually have 1 billion updates.\n",
    "\n",
    "The trick is that, in SGD you **do not need to have 1 billion iterations/updates, but much less iterations/updates**, say 1 million, and you will have \"good enough\" model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Support Vector Machines are machine learning classifiers which, given labeled training data (supervised learning), compute an optimal hyperplane which categorize new examples. <br>\n",
    "The optimal hyperplane is the one that maximizes its margin, i.e., the distance between itself and the nearest point to classify.\n",
    "\n",
    "The support vectors are the points closest to the separation hyperplane; if all other points were removed and learning re-run the result would be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
