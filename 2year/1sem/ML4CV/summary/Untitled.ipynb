{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 1 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **KNN**\n",
    "    - how it works\n",
    "    - *k* is a hyper-parameter, is not something we learn from the data, we manually set before trining starts.\n",
    "    - *model capacity*: by changing *k*, we change the model capacity, namely a measurement of **how complex/flexible** the functions that can be learned are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Linear classifiers**\n",
    "    - $f(x; \\theta) = Wx + b = scores$\n",
    "    - *as template matching*: we can think each row of the W matrix as being a template on the input image for a particular class, and out of this template matching we get a score for a particular class.\n",
    "    - *loss*: the loss is a proxy measure that is correlated with \"how good our model is\"\n",
    "        - 0-1 loss: this loss takes into accoun the number of errors that our classifier does. However this loss is difficult to optimize because produces jumps/sudden changes becuase if we move our decision boundary a bit, then the loss will probably have the exact same value.\n",
    "    - *softmax*: is an activation function that transforms scores computed by the model into probabilities (generating a probability distribution over the predictions)\n",
    "        - $softmax(scores_j) = \\frac{exp(s_j)}{sum(exp(s_{all}))}$\n",
    "    - *cross-entropy loss*: is the $-log(softmax(f(x;\\theta)))$. <br> If the true classe is \"bird\" and we have a prediction of 0.9 in correspondence of the bird, then we will have $-log(0.9) = 0.1$ which is a low value, which means that the loss will contribute just 0.1 to the total loss. If instead the true label is \"car\" and we have a prediction of 0.09 in correspondence of \"car\", then we need to penalize this because we need a high prediction in correspondence of the car, in fact $-log(0.09) = 2.4$ will give us a high value.\n",
    "    - *gradient descent*:\n",
    "        - for each epoch:\n",
    "            - forward pass: classify all training data and compute the loss\n",
    "            - backward pass: compute the gradients wrt parameteres\n",
    "            - step: update the parameters subtracting from the parameters the inverse of the gradients multiplied by a learning rate\n",
    "        - *problems*: for just one update, we perform $D_n$ (number of training example) forward and backward passes. So we take an a further appriximation of the gradient which is computed on a batch of images (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Optimizers**\n",
    "- SGD\n",
    "    - SGD with minibatches: we compute the gradient and then update the parameters only after one batch is processed.\n",
    "    - online learning: is SGD with minibatches with *batch_size* = 1\n",
    "    - *batch size* became an hyper-parameter:\n",
    "        - larger batches provide smoother estimation of the gradient and exploit hardware parallelism\n",
    "        - smaller batch size may have a regularization effect\n",
    "    - advantages: SGD with minibatches is faster because we do more updates even though the gradient is more approximate wrt standard GD.\n",
    "    - problems of SGD:\n",
    "        - producese \"sphere\" within loss landscape which which enjoy faster convergence by measn of *higher learning rates*, but we usually have small learning rates\n",
    "        - gradients estimated by mini-batches are noisy\n",
    "        - *critical points* where the gradient is 0 other tha global optima:\n",
    "            - saddle points\n",
    "            - local optima\n",
    "- Momentum\n",
    "    - momentum is like the interpretation of our parameteres moving on the landscape of our loss with the detail that **when it moves, it gains velocity**. \"A ball rolling down the surface of the loss, should keep the velocity it gains\" and this should help to navigate the loss landscape better and faster.\n",
    "    - $v^{(t + 1)}  = \\beta v^{(t)} - lr \\nabla L(\\theta^{(t)})$\n",
    "    - $\\theta^{(t+1)} = \\theta^{(t)} + v^{(t + 1)}$\n",
    "    - $\\beta$ is a value strictly less than 1\n",
    "    - $v^{(t + 1)}$ contains a running average of the previous update steps (if before we were at a certain velocity $x$, our next velocity will depends on the previous velocity)\n",
    "    - *advantages*: \n",
    "        - momentum reduces the effect of noise\n",
    "        - faster convergence\n",
    "\n",
    "- Nesterov momentum\n",
    "    - very similar to *momentum*, but the gradient is computed after having \"partially\" updated $\\theta^{(t)}$ with $\\beta v^{(t)}$:\n",
    "    - $v^{(t + 1)}  = \\beta v^{(t)} - lr \\nabla L(\\theta^{(t)} + \\beta v^{(t)})$\n",
    "    - $\\theta^{(t+1)} = \\theta^{(t)} + v^{(t + 1)}$\n",
    "    - Nesterov momentum shows a faster convergence\n",
    "- AdaGrad\n",
    "    -Adaptive Gradient proposed to rescale each entry of the gradient with the inverse of the history of the squared gradients\n",
    "    - $s^{(t + 1)} = s^{(t)} + \\nabla L (\\theta ^{(t)}) * \\nabla L (\\theta ^{(t)})$, this $s$ is the history of the squared gradients\n",
    "    - $\\theta ^{(t + 1)} =  \\theta ^{(t)} - \\frac{lr}{\\sqrt{s^{(t+1)}}+ \\epsilon} * \\nabla L (\\theta ^{(t)})$\n",
    "    - $s^{(t)}$ is monotonically increasing: it  may reduce the learning rate too early even when we are far from a good minimum\n",
    "- RMSProp\n",
    "    - in practice we do not use AdaGrad, but we use a modification of it which is RMSProp. The idea is: since $s^{(t)}$ is growing a lot, let's down-weight a bit all its history. So, instead of just accumulating square gradients into $s$, we create an exponential moving average of the square gradients. In practice we take a lot of the past history (using a $\\beta$ parameter like 0.9) and a *tiny contribution from the present values of the square gradients (1 - \\beta) in order to prevent the history to grow indefinetely.*\n",
    "    - this turned out to work better because the optimizer keeps being active: it react to changes in the loss. However it is a bit nervous, we will see that ADAM will handle this.\n",
    "    - $s^{(t + 1)} = \\beta s^{(t)} + (1 - \\beta)\\nabla L (\\theta ^{(t)}) * \\nabla L (\\theta ^{(t)})$, this $s$ is the history of the squared gradients\n",
    "    - $\\theta ^{(t + 1)} =  \\theta ^{(t)} - \\frac{lr}{\\sqrt{s^{(t+1)}}+ \\epsilon} * \\nabla L (\\theta ^{(t)})$\n",
    "    - $\\beta$ typically = $0.9$ or higher\n",
    "    \n",
    "- ADAM\n",
    "    - ADAM follows the idea of RMSProp where we create an exponential moving average of the square gradients in order to prevent the history $s$ to grow indefinitely and then uses this history to adapt the learning rate. Plus, we do the same for the gradients, so we keep an exponential moving average also for the gradiets itself. This leads ADAM into a smoother path and less nervous because we are smoothing the gradients itself.\n",
    "    - **bias correction**: since $g^{(0)} = s^{(0)} = 0$ (namely ADAM uses more of the history - which will be zero - than the current gradient while performing a step), the first values of $g$ and $s$ will be very small because it will be $0 * 0.9 + (1 - 0.9) * gradient$, so only 0.1 part of the gradient will contribiute to the first step which will result in a slow start of the optimizer. To counter this a bias correction is added to both the gradient and the history of the square gradients.\n",
    "    - $g^{(t + 1)} = \\beta_1 g^{(t)} + (1 - \\beta_1)\\nabla L (\\theta ^{(t)})$\n",
    "    - $s^{(t + 1)} = \\beta_2 s^{(t)} + (1 - \\beta_2)\\nabla L (\\theta ^{(t)}) * \\nabla L (\\theta ^{(t)})$\n",
    "    - $g^{debiased} = \\frac{g^{(t+1)}}{1-\\beta_1^{t+1}}$, $s^{debiased} = \\frac{s^{(t+1)}}{1-\\beta_2^{t+1}}$\n",
    "    - $\\theta^{(t+1)} = \\theta^{(t)} - \\frac{lr}{\\sqrt{s^{debiased}}+ \\epsilon} * g^{debiased}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Activation function**\n",
    "    - what a ReLU adds? It improves the chances that the new representation is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution\n",
    "    - properties\n",
    "    - formula of learnable parameters\n",
    "    - formula of MB to store in memory\n",
    "    - formula of flops\n",
    "- input image -- conv2D -- output shapes relationships (also for multiple layers)\n",
    "- formula for H_out and W_out\n",
    "    - after a conv2D\n",
    "    - after a conv2D with padding\n",
    "    - after a conv2D with padding and stride\n",
    "- pooling layers\n",
    "    - pro and cons\n",
    "    - formula of learnable parameters\n",
    "    - formula of MB to store in memory\n",
    "    - formula of flops\n",
    "- Batch Normalization\n",
    "    - internal covariance shift\n",
    "    - training time\n",
    "    - test time\n",
    "    - pro and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AlexNet\n",
    "    - trends\n",
    "    - general performance\n",
    "- ZFNet / Clarify\n",
    "    - (visualization of kernels and activations)\n",
    "    - general performance\n",
    "- VGG\n",
    "    - stages\n",
    "    - its three main choices\n",
    "    - (no stemming)\n",
    "    - general performance\n",
    "- Inception v1\n",
    "    - (stem - inception modules - GlobalAVGPooling)\n",
    "    - naive inception module\n",
    "    - 1x1 conv\n",
    "    - real inception module\n",
    "    - GlobalAVGPooling\n",
    "    - Inception v3\n",
    "    - general performance\n",
    "- Residual Networks\n",
    "    - residual block\n",
    "        - how is formed\n",
    "            - skip connections\n",
    "            - two 3x3 conv\n",
    "        - halve spatial resolution, doubles channels\n",
    "        - uses stem and GlobalAVGPooling\n",
    "    - skip connection dimension problem\n",
    "        - problem of 3/4 discarded pixel of the 1x1 conv with s=2\n",
    "            - solved by adding a 2x2 AvgPool layer before the 1x1 conv with s=2\n",
    "    - bottleneck residual block\n",
    "    - effects of residual learning\n",
    "- ResNeXt\n",
    "    - idea\n",
    "    - argue the growing complexity of 3x3 convs\n",
    "        - compute flops and solve for *d*\n",
    "    - why ResNeXt idea is a good one? \n",
    "    - grouped convs\n",
    "- SENet\n",
    "    - capture global context\n",
    "    - squeeze part: GlobalAVGPooling\n",
    "    - excitation part: outputs weight to reweight the channels\n",
    "        - *r* reductionf factor\n",
    "        - relative importance of channels\n",
    "- Depthwise Separable conv\n",
    "    - extreme grouped conv with #groups==C\n",
    "- Inverted residual block\n",
    "    - why Bottleneck residual block are not ok\n",
    "    - expansion - process - compression\n",
    "    - *t* expansion rate\n",
    "    - MobileNet-v2\n",
    "        - stack of inverted residual block\n",
    "- Wide ResNet\n",
    "    - ResNet with channels multiplied by a facto *k*\n",
    "- EfficientNet\n",
    "    - \"what is the optimal way to scale up a model?\"\n",
    "    - single dimension scaling\n",
    "        - all three saturates at 80%\n",
    "    - compound scaling: scaling W, D and R in an optimal way to improve the most we can\n",
    "        - compound scaling $\\phi$\n",
    "        - formulation\n",
    "    - NAS (Neaural Architecture Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model capacity\n",
    "    - factors the infuences it\n",
    "- regularization\n",
    "    - increase bias paying training error\n",
    "- parameter norm penalties\n",
    "    - optimize another term of the loss which is conflicting that say:\n",
    "        - \"we want our params to have small values\"\n",
    "        - Lambda hyperparameter\n",
    "    - weigh decay\n",
    "- early stopping\n",
    "- label smoothing\n",
    "    - problem of one hot encoding of labels\n",
    "        - making model overly confident: overfitting\n",
    "    - better alternative: smooth the labels\n",
    "        - this accounts for mislabeled examples\n",
    "    - how to apply labels smoothing\n",
    "    - KLDiv loss\n",
    "- dropout\n",
    "    - in forward pass we use a subset of the network\n",
    "        - hyperparameter *p* zeroing activation\n",
    "    - why is this a good idea?\n",
    "        - prevents feature detectors to co-adapt\n",
    "            - face detector example\n",
    "    - test time preds are stochastic\n",
    "        - value at test time\n",
    "        - expected value at training time with p=0.5\n",
    "            - example\n",
    "            - inverted drop out\n",
    "- data augmentation\n",
    "    - multi-scale training\n",
    "    - multi-scale testing\n",
    "        - domain shift problem\n",
    "        - second alternative to multi-scale testing\n",
    "- color augmentation (jittering)\n",
    "- cutout\n",
    "- Mixup\n",
    "    - linear combination of two images according to a weight lambda\n",
    "        - lambda picked from a Beta distribution\n",
    "    - why is a good idea?\n",
    "        - contraints what the network does between classes\n",
    "    - testing\n",
    "        - unmodified input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning rate schedule\n",
    "    - step decay \n",
    "    - cosine decay\n",
    "    - linear\n",
    "    - warm-up\n",
    "        - to use when our trainig loss is flattened for a long time\n",
    "    - one cycle\n",
    "        - update the learning rate after each interation, not epoch\n",
    "        - vary momentum\n",
    "- random hyper-parameter search\n",
    "- recipe to train a NN\n",
    "    - test time: ensemble\n",
    "    - snaphot ensambling\n",
    "        - uses cyclic cosine decay\n",
    "        - majory voting at test time of M models\n",
    "    - Polyak average\n",
    "        - eponential moving average of parameters\n",
    "    - Stochastic Weight Averaging\n",
    "        - uses cyclic learning rates\n",
    "        - real running average only when the learning rate is decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer Learning\n",
    "    - First way\n",
    "        - freeze backbone and train just the last layer\n",
    "    - Second way\n",
    "        - train everything\n",
    "            - discrepancy between last layer and backbone\n",
    "            - keep frozen backbne for few epochs untul last layer goes into a good landscape\n",
    "            - unfreeze backbone and train with e-4 lr if it was e-3\n",
    "            - Progressive LRs: first layers are ok so we freeze them\n",
    "                - a growing lr when we go deep into the net to be more task specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Detecting multiple objects\n",
    "    - problem 1: background\n",
    "    - problem 2: too many possible windows\n",
    "    - solution: region proposal\n",
    "        - apply with Selective Search to come up with regions that are likely to contain obj\n",
    "- R-CNN\n",
    "    - run Selective Search to come up with for example 2000 proposals\n",
    "    - for each of this proposal:\n",
    "        - warp it adding 16 pixels of context\n",
    "        - pass through the Net\n",
    "        - get class and BB correction\n",
    "    - problem: really slow\n",
    "- Fast R-CNN\n",
    "    - still run Selective Search to come up with for example 2000 proposals\n",
    "    - run full image up to a certain conv layer (like conv5) only once\n",
    "    - project the proposal into the resulting activation\n",
    "    - use RolPool layer to crop the projections and resize to the right shape\n",
    "    - advantage: the 2000 proposals pass only to a small part and non-expensive of the net\n",
    "        - which are the FC layers at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTIONS:\n",
    "- dilated convs\n",
    "    - why are useful, its advantages\n",
    "- what algo do we use to train NN\n",
    "    - what are the hyperparameters that influences the training (learning rate, batch size)\n",
    "    - effect of learning rate\n",
    "    - effect of smaller and bigger batch size\n",
    "- regularization\n",
    "    - approaches we use to improve it\n",
    "        - labels smoothing\n",
    "            - why is useful, how it works\n",
    "            - softmax and CE formula\n",
    "- metric learning\n",
    "    - why we need triplette loss, what it is and what it improves\n",
    "    - contrastive loss vs triplette loss\n",
    "    - triplette loss formula\n",
    "    - do we take all possible triplettes or just a subset?\n",
    "        - semi hard negative mining\n",
    "            - how we define an example being semi hard negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
