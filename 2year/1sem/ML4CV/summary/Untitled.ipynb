{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 1 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN\n",
    "    - model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear classifiers\n",
    "    - as template matching\n",
    "    - loss\n",
    "        - 0-1 loss\n",
    "    - softmax\n",
    "    - cross-entropy loss\n",
    "    - gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD\n",
    "    - SGD with minibatches\n",
    "    - online learning\n",
    "    - problems of SGD\n",
    "- Momentum\n",
    "- Nesterov momentum\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- representation learning\n",
    "- activation function\n",
    "    - its purpose\n",
    "- Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution\n",
    "    - properties\n",
    "    - formula of learnable parameters\n",
    "    - formula of MB to store in memory\n",
    "    - formula of flops\n",
    "- input image -- conv2D -- output shapes relationships (also for multiple layers)\n",
    "- formula for H_out and W_out\n",
    "    - after a conv2D\n",
    "    - after a conv2D with padding\n",
    "    - after a conv2D with padding and stride\n",
    "- pooling layers\n",
    "    - pro and cons\n",
    "    - formula of learnable parameters\n",
    "    - formula of MB to store in memory\n",
    "    - formula of flops\n",
    "- Batch Normalization\n",
    "    - internal covariance shift\n",
    "    - training time\n",
    "    - test time\n",
    "    - pro and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AlexNet\n",
    "    - trends\n",
    "    - general performance\n",
    "- ZFNet / Clarify\n",
    "    - (visualization of kernels and activations)\n",
    "    - general performance\n",
    "- VGG\n",
    "    - stages\n",
    "    - its three main choices\n",
    "    - (no stemming)\n",
    "    - general performance\n",
    "- Inception v1\n",
    "    - (stem - inception modules - GlobalAVGPooling)\n",
    "    - naive inception module\n",
    "    - 1x1 conv\n",
    "    - real inception module\n",
    "    - GlobalAVGPooling\n",
    "    - Inception v3\n",
    "    - general performance\n",
    "- Residual Networks\n",
    "    - residual block\n",
    "        - how is formed\n",
    "            - skip connections\n",
    "            - two 3x3 conv\n",
    "        - halve spatial resolution, doubles channels\n",
    "        - uses stem and GlobalAVGPooling\n",
    "    - skip connection dimension problem\n",
    "        - problem of 3/4 discarded pixel of the 1x1 conv with s=2\n",
    "            - solved by adding a 2x2 AvgPool layer before the 1x1 conv with s=2\n",
    "    - bottleneck residual block\n",
    "    - effects of residual learning\n",
    "- ResNeXt\n",
    "    - idea\n",
    "    - argue the growing complexity of 3x3 convs\n",
    "        - compute flops and solve for *d*\n",
    "    - why ResNeXt idea is a good one? \n",
    "    - grouped convs\n",
    "- SENet\n",
    "    - capture global context\n",
    "    - squeeze part: GlobalAVGPooling\n",
    "    - excitation part: outputs weight to reweight the channels\n",
    "        - *r* reductionf factor\n",
    "        - relative importance of channels\n",
    "- Depthwise Separable conv\n",
    "    - extreme grouped conv with #groups==C\n",
    "- Inverted residual block\n",
    "    - why Bottleneck residual block are not ok\n",
    "    - expansion - process - compression\n",
    "    - *t* expansion rate\n",
    "    - MobileNet-v2\n",
    "        - stack of inverted residual block\n",
    "- Wide ResNet\n",
    "    - ResNet with channels multiplied by a facto *k*\n",
    "- EfficientNet\n",
    "    - \"what is the optimal way to scale up a model?\"\n",
    "    - single dimension scaling\n",
    "        - all three saturates at 80%\n",
    "    - compound scaling: scaling W, D and R in an optimal way to improve the most we can\n",
    "        - compound scaling $\\phi$\n",
    "        - formulation\n",
    "    - NAS (Neaural Architecture Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model capacity\n",
    "    - factors the infuences it\n",
    "- regularization\n",
    "    - increase bias paying training error\n",
    "- parameter norm penalties\n",
    "    - optimize another term of the loss which is conflicting that say:\n",
    "        - \"we want our params to have small values\"\n",
    "        - Lambda hyperparameter\n",
    "    - weigh decay\n",
    "- early stopping\n",
    "- label smoothing\n",
    "    - problem of one hot encoding of labels\n",
    "        - making model overly confident: overfitting\n",
    "    - better alternative: smooth the labels\n",
    "        - this accounts for mislabeled examples\n",
    "    - how to apply labels smoothing\n",
    "    - KLDiv loss\n",
    "- dropout\n",
    "    - in forward pass we use a subset of the network\n",
    "        - hyperparameter *p* zeroing activation\n",
    "    - why is this a good idea?\n",
    "        - prevents feature detectors to co-adapt\n",
    "            - face detector example\n",
    "    - test time preds are stochastic\n",
    "        - value at test time\n",
    "        - expected value at training time with p=0.5\n",
    "            - example\n",
    "            - inverted drop out\n",
    "- data augmentation\n",
    "    - multi-scale training\n",
    "    - multi-scale testing\n",
    "        - domain shift problem\n",
    "        - second alternative to multi-scale testing\n",
    "- color augmentation (jittering)\n",
    "- cutout\n",
    "- Mixup\n",
    "    - linear combination of two images according to a weight lambda\n",
    "        - lambda picked from a Beta distribution\n",
    "    - why is a good idea?\n",
    "        - contraints what the network does between classes\n",
    "    - testing\n",
    "        - unmodified input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning rate schedule\n",
    "    - step decay \n",
    "    - cosine decay\n",
    "    - linear\n",
    "    - warm-up\n",
    "        - to use when our trainig loss is flattened for a long time\n",
    "    - one cycle\n",
    "        - update the learning rate after each interation, not epoch\n",
    "        - vary momentum\n",
    "- random hyper-parameter search\n",
    "- recipe to train a NN\n",
    "    - test time: ensemble\n",
    "    - snaphot ensambling\n",
    "        - uses cyclic cosine decay\n",
    "        - majory voting at test time of M models\n",
    "    - Polyak average\n",
    "        - eponential moving average of parameters\n",
    "    - Stochastic Weight Averaging\n",
    "        - uses cyclic learning rates\n",
    "        - real running average only when the learning rate is decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer Learning\n",
    "    - First way\n",
    "        - freeze backbone and train just the last layer\n",
    "    - Second way\n",
    "        - train everything\n",
    "            - discrepancy between last layer and backbone\n",
    "            - keep frozen backbne for few epochs untul last layer goes into a good landscape\n",
    "            - unfreeze backbone and train with e-4 lr if it was e-3\n",
    "            - Progressive LRs: first layers are ok so we freeze them\n",
    "                - a growing lr when we go deep into the net to be more task specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Detecting multiple objects\n",
    "    - problem 1: background\n",
    "    - problem 2: too many possible windows\n",
    "    - solution: region proposal\n",
    "        - apply with Selective Search to come up with regions that are likely to contain obj\n",
    "- R-CNN\n",
    "    - run Selective Search to come up with for example 2000 proposals\n",
    "    - for each of this proposal:\n",
    "        - warp it adding 16 pixels of context\n",
    "        - pass through the Net\n",
    "        - get class and BB correction\n",
    "    - problem: really slow\n",
    "- Fast R-CNN\n",
    "    - still run Selective Search to come up with for example 2000 proposals\n",
    "    - run full image up to a certain conv layer (like conv5) only once\n",
    "    - project the proposal into the resulting activation\n",
    "    - use RolPool layer to crop the projections and resize to the right shape\n",
    "    - advantage: the 2000 proposals pass only to a small part and non-expensive of the net\n",
    "        - which are the FC layers at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTIONS:\n",
    "- dilated convs\n",
    "    - why are useful, its advantages\n",
    "- what algo do we use to train NN\n",
    "    - what are the hyperparameters that influences the training (learning rate, batch size)\n",
    "    - effect of learning rate\n",
    "    - effect of smaller and bigger batch size\n",
    "- regularization\n",
    "    - approaches we use to improve it\n",
    "        - labels smoothing\n",
    "            - why is useful, how it works\n",
    "            - softmax and CE formula\n",
    "- metric learning\n",
    "    - why we need triplette loss, what it is and what it improves\n",
    "    - contrastive loss vs triplette loss\n",
    "    - triplette loss formula\n",
    "    - do we take all possible triplettes or just a subset?\n",
    "        - semi hard negative mining\n",
    "            - how we define an example being semi hard negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
